---
title: "Python SDK"
description: "Setting up Langtrace Python SDK with OTEL Collector"
---

## Overview

This guide will walk you through the steps to setup Langtrace's python SDK with OpenTelemetry (OTEL) Collector using.

<Note>
The Python OpenTelemetry Collector can only encode data in protobuf format. The Langtrace client processes only json encoded data. As a result the OpenTelemetry Collector cannot be used to send traces to Langtrace Cloud. You can use the OpenTelemetry Collector to send traces generated by the python SDK to an OpenTelemetry backend that supports the protobuf protocol.
</Note>

## Configurations

### OpenTelemetry Collector Configuration

When you are running your own OTEL Collector, create an `otel.yaml` file with the following configuration. This file will be mounted by the OpenTelemetry collector. When you send traces to your locally running OTEL Collector at the endpoint **http://localhost:4318/v1/traces**, the traces can then be redirected to any observability backend.

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

exporters:
  logging:
    loglevel: debug

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [logging]
```

### Python SDK Configuration
To send traces from your python project to the collector, use the following code snippet.

Install the OpenTelemetry exporter package:
```bash
pip install opentelemetry-exporter-otlp-proto-http
```

## Implementation
Initialize Langtrace SDK with the custom remote exporter that uses the OTLPTraceExporter to send traces.

```python
from langtrace_python_sdk import langtrace
from openai import OpenAI
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter


# Configure the OTLP exporter to use the correct endpoint and API key
otlp_endpoint = "http://localhost:4318/v1/traces"
otlp_exporter = OTLPSpanExporter(
    endpoint=otlp_endpoint,
    headers=(("Content-Type", "application/json"),))
langtrace.init(custom_remote_exporter=otlp_exporter, batch=False)


def chat_with_openai():
    client = OpenAI()
    messages = [
        {
            "role": "user",
            "content": "Hello, I'm a human.",
        },
    ]
    chat_completion = client.chat.completions.create(
        messages=messages,
        stream=False,
        model="gpt-3.5-turbo",
    )
    print(chat_completion.choices[0].message.content)


def main():
    chat_with_openai()


if __name__ == "__main__":
    main()
```


## Conclusion
By following this guide, you will have OpenTelemetry support added to your Python project, enabling you to send traces to an OpenTelemetry backend.
