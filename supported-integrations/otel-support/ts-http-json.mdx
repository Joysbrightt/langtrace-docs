---
title: "TypeScript SDK"
description: "Setting up Langtrace Typescript SDK with OTEL Collector"
---

## Overview

This guide will walk you through the steps to setup Langtrace with OpenTelemetry (OTEL) Collector using the HTTP/JSON protocol in a TypeScript project.

## Configurations

### OpenTelemetry Collector Configuration

When you are running your own OTEL Collector, create an `otel.yaml` file with the following configuration. This file will be mounted by the OpenTelemetry collector. When you send traces to your locally running OTEL Collector at the endpoint **http://localhost:4318/v1/traces**, the traces then get redirected to Langtrace Cloud.

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        include_metadata: true
        endpoint: 0.0.0.0:4318

exporters:
  otlphttp:
    endpoint: "https://langtrace.ai/api/trace"
    encoding: json
    compression: none
    tls:
      insecure: true
    headers:
      "x-api-key": "<LANGTRACE_API_KEY>"
      "Content-Type": "application/json"
  debug:
    verbosity: detailed

service:
  pipelines:
    traces:
      receivers:
        - otlp
      exporters:
        - debug
        - otlphttp
```

#### Important Configuration Notes
* endpoint: Change the endpoint in the otlphttp exporter section to https://<SELF_HOSTED_URL>/api/trace if you are sending your traces to self hosted Langtrace backend.
* x-api-key: The value of x-api-key should be the API key generated for your specific Langtrace project.

### TypeScript SDK Configuration
To send traces from your TypeScript project to Langtrace, use the following code snippet. Update the API key and endpoint as necessary.

Install the OpenTelemetry HTTP/JSON exporter package:
```bash
npm install @opentelemetry/exporter-trace-otlp-http
```

## Implementation
Initialize Langtrace SDK with the custom remote exporter that uses the OTLPTraceExporter to send traces to Langtrace Cloud or your self-hosted Langtrace instance.

```typescript
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-http";
import * as Langtrace from "@langtrase/typescript-sdk";
import OpenAI from "openai";

Langtrace.init({
  //   write_spans_to_console: true,
  custom_remote_exporter: new OTLPTraceExporter({
    url: process.env.LANGTRACE_API_HOST,
    headers: { 'x-api-key': process.env.LANGTRACE_API_KEY }
  })
})

const openai = new OpenAI()

export async function example (): Promise<void> {
  const completion = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [
      {
        role: 'system',
        content: 'How many states of matter are there?'
      }
    ]
  })
  console.log(completion.choices[0])
}

```

<Note>
The Langtrace client processes only json encoded data. Please make sure to use an exporter that can encode the data in json format and does not use any compression.
</Note>

## Conclusion
By following this guide, you will have OpenTelemetry support added to your TypeScript project, enabling you to send traces to Langtrace Cloud or your self-hosted Langtrace instance using the HTTP/JSON protocol.
