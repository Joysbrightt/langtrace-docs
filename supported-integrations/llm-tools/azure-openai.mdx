---
title: "Azure-OpenAI"
description: "Azure OpenAI Service integrates OpenAI's advanced AI models, like GPT-4, with Azure's
secure cloud infrastructure, enabling businesses to enhance their applications with sophisticated
language processing capabilities. It provides scalable, customizable, and enterprise-grade AI solutions while ensuring data privacy and compliance. This service leverages Azure's reliability and security to offer powerful AI functionalities across various use cases."

---

Using Langtrace to monitor your Azure-OpenAI LLM is quick and easy. Follow these steps:

## Setup

1. Install Langtrace's SDK and [initialize](/quickstart) the SDK in your code.

Note: You will also sign up and get an API key from AZURE OPENAI SERVICE [initialize](https://azure.microsoft.com/en-us/free/ai-services/?azure-portal=true/)  if you haven't done so already.

      You wil also need a deployment and Azure OPEN AI Endpoint.

2. Install packages and setup environment:

```bash Python
# Install the SDK
pip install -U langtrace-python-sdk openai
```
## Usage

Here's a quick example of how to use Langtrace with Azure-OpenAI:

```python Python
# Imports import os

import os
os.environ['LANGTRACE_API_KEY'] = "YOUR_LANGTRACE_API_KEY"
os.environ['AZURE_OPENAI_ENDPOINT'] = 'YOUR_AZURE_OPENAI_ENDPOINT'
os.environ['AZURE_OPENAI_API_KEY'] = 'YOUR_AZURE_OPENAI_API_KEY'
os.environ['API_VERSION'] = 'YOUR_API_VERSION',
os.environ['DEPLOYMENT_NAME'] = 'YOUR_DEPLOYMENT_NAME'
```

Imports

<CodeGroup>
  ```python Python
 from langtrace_python_sdk import langtrace # Must precede any llm module imports
langtrace.init(api_key = os.environ['LANGTRACE_API_KEY'])
from openai import AzureOpenAI
client = AzureOpenAI(
    api_key=os.environ["AZURE_OPENAI_API_KEY"],
    api_version=os.environ['API_VERSION'],
    azure_endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
    )
deployment_name= os.environ['DEPLOYMENT_NAME']
print('Sending a test completion job')
start_phrase = 'Write a tagline for an ice cream shop. '
response = client.completions.create(model=deployment_name, prompt=start_phrase, max_tokens=10)
print(start_phrase+response.choices[0].text)
```

    </CodeGroup>

4. Generate a simple output with your deployment's model:

<CodeGroup>
     ```bash Python
response = client.chat.completions.create(
    model="test", # model = "deployment_name".
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Does Azure OpenAI support customer managed keys?"},
        {"role": "assistant", "content": "Yes, customer managed keys are supported by Azure OpenAI."},
        {"role": "user", "content": "Do other Azure AI services support this too?"}
    ]
)

print(response.choices[0].message.content)
 ```

</CodeGroup>


You can now view your traces on the Langtrace dashboard:
 ![traces](/images/azure-openai.png)

Want to see more supported methods? Checkout the sample code in the [Langtrace Azure-OpenAI Python Example](https://github.com/Scale3-Labs/langtrace-recipes/blob/main/integrations/language-model/azure-openai/starter.ipynb) repository.